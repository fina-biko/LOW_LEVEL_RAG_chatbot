{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59b41cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "filepath=r\"C:\\Users\\User\\Downloads\\Cloud Concepts [Slides].pdf\"\n",
    "object=Path(filepath)\n",
    "print(object.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c43471ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in c:\\users\\user\\anaconda3\\envs\\alx\\lib\\site-packages (6.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\user\\anaconda3\\envs\\alx\\lib\\site-packages\\components-0.0.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\user\\anaconda3\\envs\\alx\\lib\\site-packages\\hamoye_project-0.0.1-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ff0ed93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'MICHELE ACHIENG, \\nP .O Box 10369-00100, \\nNAIROBI. \\n \\n \\nTO THE HEADTEACHER KILIMANI JUNIOR SCHOOL \\nP .O BOX 21557-00505 \\nNAIROBI \\n \\n \\nDear sir/Madam \\nRE: APPLICATION FOR A PRIMARY TEACHER SCHOOL POSITION \\nI wish to apply for the position of a pri mary teacher in your school. I am a trained P1 \\nteacher and I am confident that my educational background and work experience skills will \\nbe of great value to your school. \\nAs a gr aduate teacher, have experienced many skills to work with a varie ty of \\nprofessional teaching team and gained valuable insight into the role of a primary school \\nteacher . Previously, I worked in a school where I managed to demonstrate excellent planning, \\ncommunication and teamwork skills. My pos ition for the young has enabl ed me to pro vide    \\nexcellent learning opportunities within each classroom I have been placed. \\nI am a dedicated teacher with a great deal of Knowledge and skills as well as passion \\nand enthusi asm for working with chi ldren and teaching in the area of primary school \\neducation. I am self motivated and can work cooper atively in a team  situation or on  own. I \\nhave always provided myself on being able to fit and get along with others. \\n \\nPlease find attached copy of my Curriculum Vitae along with my documents. Do not hesitate \\nto contact me as at your school convenience as I look forward to further discussing this \\napplication with you. \\nThank you for taking your time to consider my suitability for your school and confidence that \\nI can bring the necessary skills.  \\n  \\n \\nYours sincerely, \\n ACHIENG MICHELE      '), (1, ' \\n \\n ')]\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "filepath=r\"C:\\Users\\User\\Desktop\\michelle.pdf\"\n",
    "reader = PdfReader(stream=filepath)\n",
    "#list contains tuples of (page_number, text)\n",
    "extracted_text:list[str]=[]\n",
    "for pair in enumerate(reader.pages):\n",
    "\n",
    "    extracted_text.append((pair[0], pair[1].extract_text()))\n",
    "\n",
    "print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01681518",
   "metadata": {},
   "source": [
    "Every list in Python already has built-in numbering (called indexes).\n",
    "\n",
    "Take your list:\n",
    "\n",
    "nums = [0, 1, 2, 3, 4]\n",
    "\n",
    "\n",
    "üëâ Python automatically assigns each item a position (index):\n",
    "\n",
    "nums[0] ‚Üí 0\n",
    "\n",
    "nums[1] ‚Üí 1\n",
    "\n",
    "nums[2] ‚Üí 2\n",
    "\n",
    "nums[3] ‚Üí 3\n",
    "\n",
    "nums[4] ‚Üí 4\n",
    "\n",
    "So the index is the number you use to access the element.\n",
    "\n",
    "If you want both the index and the value at the same time, you use enumerate:\n",
    "\n",
    "for i, value in enumerate(nums):\n",
    "    print(f\"Index {i} has value {value}\")\n",
    "\n",
    "\n",
    "Output:\n",
    "\n",
    "Index 0 has value 0\n",
    "Index 1 has value 1\n",
    "Index 2 has value 2\n",
    "Index 3 has value 3\n",
    "Index 4 has value 4\n",
    "\n",
    "\n",
    "So to answer you in simple terms:\n",
    "\n",
    "The list itself stores the values (0, 1, 2, 3, 4).\n",
    "\n",
    "Python automatically numbers them with indexes starting from 0.\n",
    "\n",
    "You can use the index to access or modify items.\n",
    "\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    print(text)\n",
    "\n",
    "by defaultdoes it print the whle two pages text as asingle string?\n",
    "ChatGPT said:\n",
    "\n",
    "Good question üëç\n",
    "\n",
    "No ‚Äî it doesn‚Äôt merge all pages into one string by default.\n",
    "\n",
    "Let‚Äôs break it down:\n",
    "\n",
    "for page in reader.pages:     # goes one page at a time\n",
    "    text = page.extract_text() # extracts text for that single page\n",
    "    print(text)                # prints it right away\n",
    "\n",
    "\n",
    "üëâ What happens:\n",
    "\n",
    "reader.pages is a list-like object where each item is one page.\n",
    "\n",
    "page.extract_text() extracts only the text of that page.\n",
    "\n",
    "Since the print() is inside the loop, Python prints one page at a time.\n",
    "\n",
    "So if your PDF has 2 pages:\n",
    "\n",
    "The first print(text) shows page 1‚Äôs text.\n",
    "\n",
    "Then the loop moves on, and the next print(text) shows page 2‚Äôs text.\n",
    "\n",
    "‚ö†Ô∏è But ‚Äî if your PDF text doesn‚Äôt have clear page breaks, it may look like one big string when printed, even though they are printed separately.\n",
    "\n",
    "If you want them clearly separated, you can do:\n",
    "\n",
    "for i, page in enumerate(reader.pages, start=1):\n",
    "    text = page.extract_text()\n",
    "    print(f\"\\n--- Page {i} ---\\n\")\n",
    "    print(text)\n",
    "\n",
    "\n",
    "That way, you‚Äôll always see where one page ends and the next starts.\n",
    "\n",
    "üëâ Do you want me to also show you how to combine all pages into a single string instead (for later processing like chunking)?\n",
    "\n",
    "You said:\n",
    "so for any list, if it has the contnet say integers, i can assign those integrs a number/index?\n",
    "say [0,1,2,3,4]\n",
    "ChatGPT said:\n",
    "\n",
    "Exactly ‚úÖ\n",
    "\n",
    "Every list in Python already has built-in numbering (called indexes).\n",
    "\n",
    "Take your list:\n",
    "\n",
    "nums = [0, 1, 2, 3, 4]\n",
    "\n",
    "\n",
    "üëâ Python automatically assigns each item a position (index):\n",
    "\n",
    "nums[0] ‚Üí 0\n",
    "\n",
    "nums[1] ‚Üí 1\n",
    "\n",
    "nums[2] ‚Üí 2\n",
    "\n",
    "nums[3] ‚Üí 3\n",
    "\n",
    "nums[4] ‚Üí 4\n",
    "\n",
    "So the index is the number you use to access the element.\n",
    "\n",
    "If you want both the index and the value at the same time, you use enumerate:\n",
    "\n",
    "for i, value in enumerate(nums):\n",
    "    print(f\"Index {i} has value {value}\")\n",
    "\n",
    "\n",
    "Output:\n",
    "\n",
    "Index 0 has value 0\n",
    "Index 1 has value 1\n",
    "Index 2 has value 2\n",
    "Index 3 has value 3\n",
    "Index 4 has value 4\n",
    "\n",
    "\n",
    "So to answer you in simple terms:\n",
    "\n",
    "The list itself stores the values (0, 1, 2, 3, 4).\n",
    "\n",
    "Python automatically numbers them with indexes starting from 0.\n",
    "\n",
    "You can use the index to access or modify items.\n",
    "\n",
    "üëâ Do you want me to also explain how negative indexes work in lists? (like nums[-1] giving the last element).\n",
    "\n",
    "You said:\n",
    "so when you itrate a list to prin tit contents , it does not print the indexs attached, unless you use enumearte?\n",
    "ChatGPT said:\n",
    "\n",
    "üí° Correct!\n",
    "\n",
    "When you loop through a list normally, Python only gives you the values, not the indexes.\n",
    "\n",
    "Example:\n",
    "\n",
    "nums = [10, 20, 30, 40]\n",
    "\n",
    "for n in nums:\n",
    "    print(n)\n",
    "\n",
    "\n",
    "Output:\n",
    "\n",
    "10\n",
    "20\n",
    "30\n",
    "40\n",
    "\n",
    "\n",
    "üëâ No indexes shown.\n",
    "\n",
    "If you also want the index + value, you use enumerate:\n",
    "\n",
    "for i, n in enumerate(nums):\n",
    "    print(f\"Index {i} ‚Üí Value {n}\")\n",
    "\n",
    "\n",
    "Output:\n",
    "\n",
    "Index 0 ‚Üí Value 10\n",
    "Index 1 ‚Üí Value 20\n",
    "Index 2 ‚Üí Value 30\n",
    "Index 3 ‚Üí Value 40\n",
    "\n",
    "\n",
    "So:\n",
    "\n",
    "Plain loop ‚Üí just values.\n",
    "\n",
    "With enumerate ‚Üí indexes + values.\n",
    "\n",
    "‚ö° Extra: If you only want indexes, you can use range(len(nums)):\n",
    "\n",
    "for i in range(len(nums)):\n",
    "    print(i, nums[i])\n",
    "\n",
    "\n",
    "This does the same thing, but enumerate is cleaner.\n",
    "\n",
    "Would you like me to show you how enumerate actually works under the hood (it‚Äôs just a small helper class)?\n",
    "\n",
    "You said:\n",
    " we had a list of syrings and we looped, how will the output look like, \n",
    "ChatGPT said:\n",
    "\n",
    "Let‚Äôs try with a list of strings and see what happens üëá\n",
    "\n",
    "words = [\"apple\", \"banana\", \"cherry\"]\n",
    "\n",
    "# Normal loop\n",
    "for word in words:\n",
    "    print(word)\n",
    "\n",
    "\n",
    "‚úÖ Output:\n",
    "\n",
    "apple\n",
    "banana\n",
    "cherry\n",
    "\n",
    "\n",
    "üëâ Each element in the list is printed on its own line, no indexes shown.\n",
    "\n",
    "Now with enumerate:\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    print(i, word)\n",
    "\n",
    "\n",
    "‚úÖ Output:\n",
    "\n",
    "0 apple\n",
    "1 banana\n",
    "2 cherry\n",
    "\n",
    "\n",
    "Here Python gives you both the index (i) and the string value (word).\n",
    "\n",
    "If you like, I can also show you what happens if you don‚Äôt unpack enumerate (so you see the raw pairs). Want me to?\n",
    "\n",
    "You said:\n",
    "yes\n",
    "ChatGPT said:\n",
    "\n",
    "Perfect üëç Let‚Äôs do it.\n",
    "\n",
    "If you don‚Äôt unpack enumerate, each iteration gives you a tuple (index, value) directly:\n",
    "\n",
    "words = [\"apple\", \"banana\", \"cherry\"]\n",
    "\n",
    "for pair in enumerate(words):\n",
    "    print(pair)\n",
    "\n",
    "\n",
    "‚úÖ Output:\n",
    "\n",
    "(0, 'apple')\n",
    "(1, 'banana')\n",
    "(2, 'cherry')\n",
    "\n",
    "\n",
    "So you see the index and the string grouped together as tuples.\n",
    "\n",
    "üëâ That‚Äôs why most of the time, people unpack it like this:\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    print(i, word)\n",
    "\n",
    "\n",
    "because it‚Äôs cleaner.\n",
    "\n",
    "Would you like me to also show you how you can start enumeration at 1 instead of 0 (sometimes useful when dealing with pages, steps, or human-friendly numbering)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86adf77",
   "metadata": {},
   "source": [
    "the book has 1000 pages, and each page and its content is stored as a tuple and returned\n",
    "so when i loop through the pages, it will extytract each page, meaning i need somewhere to store the all the extraceted text together, like [(),(),(),() ....] , something like a list, but the content is large , the list will be \"heavy\", what do you think  should do? continue storing the tuples inside the list, list of tuples or ?\n",
    "ChatGPT said:\n",
    "\n",
    "Good thinking üëå you‚Äôve spotted a real design trade-off. Let‚Äôs break it down in layman terms:\n",
    "\n",
    "1. What‚Äôs happening\n",
    "\n",
    "You have a PDF with 1000 pages.\n",
    "\n",
    "Each page is like a (page_number, page_text) tuple.\n",
    "\n",
    "If you loop, you‚Äôll end up with something like:\n",
    "\n",
    "[\n",
    "  (1, \"This is page 1 text...\"),\n",
    "  (2, \"This is page 2 text...\"),\n",
    "  ...\n",
    "  (1000, \"This is page 1000 text...\")\n",
    "]\n",
    "\n",
    "\n",
    "That‚Äôs a list of tuples. It works fine.\n",
    "\n",
    "2. The \"heavy list\" concern\n",
    "\n",
    "A Python list can handle 1000 items easily. Even 10,000 is fine.\n",
    "\n",
    "The ‚Äúheaviness‚Äù comes not from the list itself, but from the text content (if each page is very long).\n",
    "\n",
    "But still: storing plain text in memory (like 1000 pages) is usually fine unless your PDF is gigantic (hundreds of MBs).\n",
    "\n",
    "3. Options\n",
    "\n",
    "Here are 3 common ways experts handle it:\n",
    "\n",
    "‚úÖ Option A: List of tuples (simple & good enough)\n",
    "pages_text = [(i, page.extract_text()) for i, page in enumerate(reader.pages, start=1)]\n",
    "\n",
    "\n",
    "Easy to use.\n",
    "\n",
    "Works well if you don‚Äôt mind holding everything in memory.\n",
    "\n",
    "‚úÖ Option B: Write to disk as you go\n",
    "\n",
    "If the PDF is huge and memory becomes an issue, don‚Äôt keep everything in RAM:\n",
    "\n",
    "with open(\"book_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, page in enumerate(reader.pages, start=1):\n",
    "        text = page.extract_text()\n",
    "        f.write(f\"--- Page {i} ---\\n{text}\\n\\n\")\n",
    "\n",
    "\n",
    "Saves page text directly to a file.\n",
    "\n",
    "Very memory-friendly.\n",
    "\n",
    "‚úÖ Option C: Generator (lazy, on-demand)\n",
    "\n",
    "Instead of making a big list, you can yield page text one by one:\n",
    "\n",
    "def extract_pages(reader):\n",
    "    for i, page in enumerate(reader.pages, start=1):\n",
    "        yield (i, page.extract_text())\n",
    "\n",
    "\n",
    "Then you can loop:\n",
    "\n",
    "for page_number, text in extract_pages(reader):\n",
    "    print(page_number, text[:100])  # preview only\n",
    "\n",
    "\n",
    "This way, you don‚Äôt store everything in memory.\n",
    "\n",
    "You process each page as it comes.\n",
    "\n",
    "üëâ If you‚Äôre just experimenting or doing NLP later, Option A (list of tuples) is fine.\n",
    "üëâ If you‚Äôre dealing with very big PDFs, Option B or C is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a768c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 0: MICHELE ACHIENG, \n",
      "P .O Box 10369-00100, \n",
      "NAIROBI. \n",
      " \n",
      " \n",
      "TO THE HEADTEACHER KILIMANI JUNIOR SCHOOL \n",
      "P .O BOX 21557-00505 \n",
      "NAIROBI \n",
      " \n",
      " \n",
      "Dear sir/Madam \n",
      "RE: APPLICATION FOR A PRIMARY TEACHER SCHOOL POSITION \n",
      "I wish to apply for the position of a pri mary teacher in your school. I am a trained P1 \n",
      "teacher and I am confident that my educational background and work experience skills will \n",
      "be of great value to your school. \n",
      "As a gr aduate teacher, have experienced many skills to work with a varie ty of \n",
      "professional teaching team and gained valuable insight into the role of a primary school \n",
      "teacher . Previously, I worked in a school where I managed to demonstrate excellent planning, \n",
      "communication and teamwork skills. My pos ition for the young has enabl ed me to pro vide    \n",
      "excellent learning opportunities within each classroom I have been placed. \n",
      "I am a dedicated teacher with a great deal of Knowledge and skills as well as passion \n",
      "and enthusi asm for working with chi ldren and teaching in the area of primary school \n",
      "education. I am self motivated and can work cooper atively in a team  situation or on  own. I \n",
      "have always provided myself on being able to fit and get along with others. \n",
      " \n",
      "Please find attached copy of my Curriculum Vitae along with my documents. Do not hesitate \n",
      "to contact me as at your school convenience as I look forward to further discussing this \n",
      "application with you. \n",
      "Thank you for taking your time to consider my suitability for your school and confidence that \n",
      "I can bring the necessary skills.  \n",
      "  \n",
      " \n",
      "Yours sincerely, \n",
      " ACHIENG MICHELE      \n",
      "Page 1:  \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1566694c",
   "metadata": {},
   "source": [
    "# clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c18d03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "\n",
    "    \"\"\"\n",
    "    Clean raw extracted text before chunking or passing to an LLM.\n",
    "\n",
    "    Steps performed:\n",
    "    1. Replace double newlines (\"\\n\\n\") with a single space\n",
    "       - Merges paragraph breaks into a flowing text.\n",
    "    2. Replace single newlines (\"\\n\") with a space\n",
    "       - Ensures lines from PDFs don‚Äôt appear broken.\n",
    "    3. Remove hyphen + newline word breaks\n",
    "       - Example: \"pri-\\nmary\" ‚Üí \"primary\".\n",
    "    4. Collapse multiple spaces into a single space\n",
    "       - Example: \"I   am   here\" ‚Üí \"I am here\".\n",
    "    5. Normalize spaces before punctuation\n",
    "       - Example: \"Hello , world !\" ‚Üí \"Hello, world!\".\n",
    "    6. Strip leading and trailing spaces\n",
    "       - Ensures clean text without unnecessary whitespace at start or end.\n",
    "\n",
    "    Args:\n",
    "        text (str): The raw extracted text.\n",
    "\n",
    "    Returns:\n",
    "        str: A cleaned, normalized version of the text.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Replace double newlines (\\n\\n) with a single space\n",
    "    text = text.replace(\"\\n\\n\", \" \")\n",
    "\n",
    "    # 2. Replace single newlines (\\n) with a space\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    # 3. Remove broken words where a hyphen is followed by a newline\n",
    "    # Example: \"pri-\\nmary\" ‚Üí \"primary\"\n",
    "    text = re.sub(r\"-\\s+\", \"\", text)\n",
    "\n",
    "    # 4. Collapse multiple spaces into a single space\n",
    "    # Example: \"I   am   here\" ‚Üí \"I am here\"\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # 5. (Optional) Normalize spaces before punctuation\n",
    "    # Example: \"Hello , world !\" ‚Üí \"Hello, world!\"\n",
    "    text = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", text)\n",
    "\n",
    "    # 6. (Optional) Strip leading/trailing spaces\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc727a81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3beda132",
   "metadata": {},
   "source": [
    "# chunk the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b510c0d",
   "metadata": {},
   "source": [
    "1.so to chunk i can either create a function that  counts number of words in the clean text, say 50 words, then in looping to the next 50 words, it should take the previous 10 words and add to it\n",
    "this way until the last, and so it returns a list of chunks\n",
    "\n",
    "then in the embedding model, we pass each each chunk to be to tokenizer to teokenize each word in the chunk, then pass the tokenized  chunk to the embedding model\n",
    "\n",
    "2.or we  get a cunking model, so this means there is a tokenizer model that also ahs the capabikitytochunk, then it returns the tokenized chunk overlapperd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. Do-it-yourself (manual chunking)\n",
    "\n",
    "You first clean the text.\n",
    "\n",
    "Write a chunking function:\n",
    "\n",
    "Split text into words (e.g., by .split()).\n",
    "\n",
    "Take the first 50 words ‚Üí make a chunk.\n",
    "\n",
    "Next chunk starts from word 40 (so there‚Äôs 10-word overlap) and goes to word 90.\n",
    "\n",
    "Continue until the end of the text.\n",
    "\n",
    "Result: a list of chunks (each chunk ~50 words, with overlap).\n",
    "\n",
    "Then:\n",
    "\n",
    "For each chunk ‚Üí send it to the tokenizer.\n",
    "\n",
    "Tokenizer converts words ‚Üí tokens (numbers).\n",
    "\n",
    "Send tokens ‚Üí embedding model or LLM.\n",
    "\n",
    "‚úÖ Advantage: You fully control chunk size, overlap, and cleaning.\n",
    "‚ö†Ô∏è Downside: More work, and you need to tune chunk size manually.\n",
    "\n",
    "2. Let the model handle it (tokenizer with chunking support)\n",
    "\n",
    "Some libraries (like LangChain, HuggingFace‚Äôs tokenizers, or OpenAI tiktoken) have built-in ways to:\n",
    "\n",
    "Tokenize text ‚Üí into tokens.\n",
    "\n",
    "Automatically split into chunks of max_tokens (e.g., 512 tokens).\n",
    "\n",
    "Add overlap between chunks.\n",
    "\n",
    "Example:\n",
    "\n",
    "Tell the tokenizer: ‚ÄúSplit into chunks of 512 tokens with 50-token overlap.‚Äù\n",
    "\n",
    "It will return:\n",
    "\n",
    "[ [token1...token512], [token462...token974], ... ]\n",
    "\n",
    "\n",
    "Each chunk is already tokenized and overlapped, ready for embeddings.\n",
    "\n",
    "‚úÖ Advantage: Easy, efficient, and consistent with the model‚Äôs tokenization rules.\n",
    "‚ö†Ô∏è Downside: Less control if you want a custom word-based approach (like exactly 50 words instead of tokens).\n",
    "\n",
    "üëâ If you want to use OpenAI embeddings or HuggingFace embeddings, I recommend Option 2:\n",
    "\n",
    "Use their tokenizer (e.g., tiktoken for OpenAI or AutoTokenizer in HuggingFace).\n",
    "\n",
    "Let the tokenizer both split into tokens and handle chunking with overlap.\n",
    "\n",
    "Then pass directly into the embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d4ee81",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb22938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 1, Content: First page text.This has a hyphenated word.\n",
      "Page: 2, Content: Second page text with a space   problem.\n",
      "[[{1: 'Fir'}, {1: 'rst'}, {1: 't p'}, {1: 'pag'}, {1: 'ge '}, {1: ' te'}, {1: 'ext'}, {1: 't.T'}, {1: 'Thi'}, {1: 'is '}, {1: ' ha'}, {1: 'as '}, {1: ' a '}, {1: ' hy'}, {1: 'yph'}, {1: 'hen'}, {1: 'nat'}, {1: 'ted'}, {1: 'd w'}, {1: 'wor'}, {1: 'rd.'}, {1: '.'}], []]\n"
     ]
    }
   ],
   "source": [
    "tuple_list = [(1, \"First page text.This has a hyphenated word.\"), (2, \"Second page text with a space   problem.\")]\n",
    "#Configuration\n",
    "start_index = 0\n",
    "CHUNK_SIZE = 3 #3 words\n",
    "OVERLAP = 1\n",
    "STEP = CHUNK_SIZE - OVERLAP  # 3 - 2 = 1 (This means the start index increases by 1 each time)\n",
    "outer_list=[]\n",
    "#supose it is a 1D\n",
    "for each_tuple in tuple_list:\n",
    "    \n",
    "    inner_list=[]\n",
    "    #get the page number\n",
    "    page=each_tuple[0]\n",
    "    content=each_tuple[1]\n",
    "    print(f\"Page: {page}, Content: {content}\")\n",
    "\n",
    "\n",
    "    while start_index<len(content):\n",
    "        chunk=content[start_index:start_index+CHUNK_SIZE]\n",
    "        inner_list.append({page:chunk})\n",
    "        start_index += STEP\n",
    "    \n",
    "    outer_list.append(inner_list)\n",
    "\n",
    "print(outer_list)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da760a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(tuple_list, chunk_size=10, overlap=3):\n",
    "    \"\"\"\n",
    "    Chunk text from a list of tuples containing (page_number, text).\n",
    "\n",
    "    Args:\n",
    "        tuple_list (list): List of tuples where each tuple is (page_number, text).\n",
    "        chunk_size (int): Number of words per chunk.\n",
    "        overlap (int): Number of overlapping words between chunks.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of lists containing dictionaries with page numbers as keys and text chunks as values.\n",
    "    \"\"\"\n",
    "    # Configuration (This is for WORDS, which requires splitting the content)\n",
    "    CHUNK_SIZE = 3 # 3 words\n",
    "    OVERLAP = 1\n",
    "    STEP = CHUNK_SIZE - OVERLAP  # 3 - 1 = 2 \n",
    "\n",
    "    outer_list = []\n",
    "\n",
    "    for each_tuple in tuple_list:\n",
    "        # üí• FIX 1: Resetting start_index for each new page is crucial\n",
    "        start_index = 0 \n",
    "        inner_list = []\n",
    "        \n",
    "        page = each_tuple[0]\n",
    "        content = each_tuple[1]\n",
    "        \n",
    "        # üí• FIX 2: Clean and split the content into WORDS\n",
    "        # Clean up non-breaking spaces (\\xa0) and extra whitespace before splitting\n",
    "        #cleaned_content = content.replace('\\xa0', ' ').replace('.', ' ').replace(',', ' ')\n",
    "        words = [word for word in content.split() if word] # Get a list of actual words\n",
    "        print(f\"this is words after splitting based on white space: {words}\")\n",
    "        \n",
    "        print(f\"\\n--- Processing Page {page} ---\")\n",
    "        print(f\"Content (Cleaned Words): {words}\")\n",
    "        \n",
    "        # üí• FIX 3: Loop condition checks against the length of the WORDS list\n",
    "        while start_index < len(words):\n",
    "            # Slice the list of WORDS, not the character string\n",
    "            chunk_list = words[start_index : start_index + CHUNK_SIZE]\n",
    "            print(f\"this is chunk list for {page}: {chunk_list}\")\n",
    "            \n",
    "            # Stop if the chunk is empty (shouldn't happen with correct logic, but a safeguard)\n",
    "            if not chunk_list:\n",
    "                break\n",
    "                \n",
    "            # Join the chunk list back into a string for the desired dictionary format\n",
    "            chunk_string = \" \".join(chunk_list)\n",
    "            inner_list.append({page: chunk_string})\n",
    "            \n",
    "            # Advance the index by the calculated STEP (which is 2 words)\n",
    "            start_index += STEP\n",
    "        \n",
    "        outer_list.append(inner_list)\n",
    "\n",
    "    print(\"\\n--- Final Result ---\")\n",
    "    print(outer_list)\n",
    "    return outer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99092a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is words after splitting based on white space: ['First', 'page', 'text.This', 'has', 'a', 'hyphenated', 'word.']\n",
      "\n",
      "--- Processing Page 1 ---\n",
      "Content (Cleaned Words): ['First', 'page', 'text.This', 'has', 'a', 'hyphenated', 'word.']\n",
      "this is chunk list for 1: ['First', 'page', 'text.This']\n",
      "this is chunk list for 1: ['text.This', 'has', 'a']\n",
      "this is chunk list for 1: ['a', 'hyphenated', 'word.']\n",
      "this is chunk list for 1: ['word.']\n",
      "this is words after splitting based on white space: ['Second', 'page', 'text', 'with', 'a', 'space', 'problem.']\n",
      "\n",
      "--- Processing Page 2 ---\n",
      "Content (Cleaned Words): ['Second', 'page', 'text', 'with', 'a', 'space', 'problem.']\n",
      "this is chunk list for 2: ['Second', 'page', 'text']\n",
      "this is chunk list for 2: ['text', 'with', 'a']\n",
      "this is chunk list for 2: ['a', 'space', 'problem.']\n",
      "this is chunk list for 2: ['problem.']\n",
      "\n",
      "--- Final Result ---\n",
      "[[{1: 'First page text.This'}, {1: 'text.This has a'}, {1: 'a hyphenated word.'}, {1: 'word.'}], [{2: 'Second page text'}, {2: 'text with a'}, {2: 'a space problem.'}, {2: 'problem.'}]]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "tuple_list = [(1, \"First page text.This has a hyphenated word.\"), (2, \"Second page text with a space \\xa0 problem.\")]\n",
    "\n",
    "# Configuration (This is for WORDS, which requires splitting the content)\n",
    "CHUNK_SIZE = 3 # 3 words\n",
    "OVERLAP = 1\n",
    "STEP = CHUNK_SIZE - OVERLAP  # 3 - 1 = 2 \n",
    "\n",
    "outer_list = []\n",
    "\n",
    "for each_tuple in tuple_list:\n",
    "    # üí• FIX 1: Resetting start_index for each new page is crucial\n",
    "    start_index = 0 \n",
    "    inner_list = []\n",
    "    \n",
    "    page = each_tuple[0]\n",
    "    content = each_tuple[1]\n",
    "    \n",
    "    # üí• FIX 2: Clean and split the content into WORDS\n",
    "    # Clean up non-breaking spaces (\\xa0) and extra whitespace before splitting\n",
    "    #cleaned_content = content.replace('\\xa0', ' ').replace('.', ' ').replace(',', ' ')\n",
    "    words = [word for word in content.split() if word] # Get a list of actual words\n",
    "    print(f\"this is words after splitting based on white space: {words}\")\n",
    "    \n",
    "    print(f\"\\n--- Processing Page {page} ---\")\n",
    "    print(f\"Content (Cleaned Words): {words}\")\n",
    "    \n",
    "    # üí• FIX 3: Loop condition checks against the length of the WORDS list\n",
    "    while start_index < len(words):\n",
    "        # Slice the list of WORDS, not the character string\n",
    "        chunk_list = words[start_index : start_index + CHUNK_SIZE]\n",
    "        print(f\"this is chunk list for {page}: {chunk_list}\")\n",
    "        \n",
    "        # Stop if the chunk is empty (shouldn't happen with correct logic, but a safeguard)\n",
    "        if not chunk_list:\n",
    "            break\n",
    "            \n",
    "        # Join the chunk list back into a string for the desired dictionary format\n",
    "        chunk_string = \" \".join(chunk_list)\n",
    "        inner_list.append({page: chunk_string})\n",
    "        \n",
    "        # Advance the index by the calculated STEP (which is 2 words)\n",
    "        start_index += STEP\n",
    "    \n",
    "    outer_list.append(inner_list)\n",
    "\n",
    "print(\"\\n--- Final Result ---\")\n",
    "print(outer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e23b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'string', 'with', 'several', 'white', 'spaces.']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "656672f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "(1, 'First page text.\\n This has a hyphen-\\nated word.')\n",
      "-----\n",
      "(2, 'Second page text with a space   problem.')\n"
     ]
    }
   ],
   "source": [
    "tuple_list = [(1, \"First page text.\\n This has a hyphen-\\nated word.\"), (2, \"Second page text with a space   problem.\")]\n",
    "for element in tuple_list:\n",
    "   print(\"-----\")\n",
    "   print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f61e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"google/embeddinggemma-300m\")\n",
    "def embed_text(outer_list,model_name=model):\n",
    "    \"\"\"\n",
    "    Embed text chunks from a list of lists containing dictionaries with page numbers as keys and text chunks as values.\n",
    "\n",
    "    Args:\n",
    "        outer_list (list): A list of lists where each inner list contains dictionaries with page numbers as keys and text chunks as values.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of lists containing dictionaries with page numbers, embedded chunks, and original chunks.\n",
    "    \"\"\"\n",
    "    # This will hold the final result\n",
    "    final_outer_list = []\n",
    "    try:\n",
    "\n",
    "        for inner_list in outer_list:\n",
    "            list_of_tokens = []\n",
    "            for first_dict in inner_list:  # the number of dict in each inner list is the number of chunks\n",
    "                # unpack the dictionary\n",
    "                page = list(first_dict.keys())[0]\n",
    "                text = list(first_dict.values())[0]  # the first chunk\n",
    "                embedded_chunk =  model_name.encode_query(text)\n",
    "                # say it returns sth\n",
    "                # append this embedded chunk and the page number and the original chunk as dict to this list\n",
    "                list_of_tokens.append({page: page, \"embedded_chunk\": embedded_chunk, \"original_chunk\": text})\n",
    "            final_outer_list.append(list_of_tokens)\n",
    "\n",
    "        return final_outer_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a930ca59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
